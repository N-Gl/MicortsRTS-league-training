defaults:
  - default_config

ExperimentConfig:
  selfplay: true
  model_path: models/BCagent.pt
  BC_model_path:        # BC_model_path == model_path, falls BC_model_path None ist
  exp_name: 12_12_25__04_12_25__BCagent
  wandb_project_name: new_PPO

  prod_mode: true
  render: false
  render_all: false

  seed: 

  num_bot_envs: 25
  num_main_agents: 0   # 1 num_main_agents == 1 env

  total_timesteps: 20000000

  early_updates: true  # TODO

  attack_reward_weight: 0.05 # (im basis ppo: 1)
  dyn_attack_reward: 0.0
  rewardscore: 0
  value_warmup_updates: -1
  # reward_weight=np.array([10.0, 1.0, 1.0, 0.2, 1.0, 4.0]) (winloss = 1)

  endgame_maps: false

  PPO_learning_rate: 2.5e-5     # new (BA Parameter) (learning rate = 2.5 × 10^−5)
  PPO_weight_decay: 0

  dbg_no_main_agent_ppo_update: false # TODO

  even_opponent_split: false



  num_steps: 512                # new (BA Parameter) (Rollout steps per environment = 512)
  gamma: 1.0                    # new (BA Parameter) Discount factor γ = 1 (no discounting)
  gae_lambda: 0.95              # new (BA Parameter) (GAE parameter λ = 0.95)
  ent_coef: 0.005               # new (BA Parameter) (Entropy coefficient η = 0.005)
  vf_coef: 0.01                 # new (BA Parameter) (Value function coefficient β = 0.01)
  max_grad_norm: 0.5            # new (BA Parameter) (Gradient norm threshold ω = 0.5)
  clip_coef: 0.1                # new (BA Parameter) (Clipping coefficient = 0.1)
  update_epochs: 4              
  kle_stop: false               
  kle_rollback: false           
  target_kl: 0.03               
  kl_coeff: 0.4                 # new (BA Parameter) (KL regularization coefficient λ_(KL) = 0.4)
  norm_adv: true                
  anneal_lr: true               
  clip_vloss: true